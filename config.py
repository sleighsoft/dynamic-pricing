from scipy.stats import randint as sk_randint
import hashlib
import base64


def calculate_merchant_id(token):
  return base64.b64encode(
      hashlib.sha256(token.encode('utf-8')).digest()).decode('utf-8')


def get_config():
  # Token generated by <host>/index.html#/deployment
  merchant_token = 'FGlXLHVdvmcGA5fWupDpSg7pF5nrj1CxGxWbG1FCosSyw8VryihRQoGfs5Xc5jaM'
  return {
      'merchant_token': merchant_token,
      # Debug settings
      'debug': True,
      # => Log model statistics e.g. MCFadden, precision, recall, ...
      'log_statistics': True,
      # # Merchant configuration # #
      'merchant_config': {
          # Routes
          'merchant_id': calculate_merchant_id(merchant_token),
          'marketplace_url': 'http://localhost:8080',
          'producer_url': 'http://localhost:3050',
          'kafka_reverse_proxy_url': 'http://localhost:8001',
          # Market configuration
          'max_amount_of_offers': 5,
          'max_req_per_sec': 4,
          'shipping': 5,
          'primeShipping': 1,
          # Learning configuration
          'seconds_between_learning': 180,
      },
      # # Pricing # #
      'price_decrement': 0.2,
      # # Training configuration # #
      # Warm-up phase configuration (#sold products per product_id)
      'warmup': 60,
      'situations_save_path': 'run/data/situations.pkl',
      'model_save_dir': 'run/models/',
      # Model parameter
      'param_save_dir': 'run/params/',
      # Feature selector
      'selector_save_dir': 'run/selectors/',
      # Number of times to perform paramter search before stopping it.
      # Parameter search becomes computationally very exepensive after a while.
      'first_n_param_search': 1,
      # Type of (over)sampling to be performed
      'sampling_mode': 'smote',
      'sampling_ratio': 0.45,
      # Feature selection
      'feature_selection': True,
      'feature_selection_threshold': 0.15,
      # # Model configuration # #
      # gbc = GradientBoostingClassifier
      # xgb = XGBoost Classifier
      'model': 'xgb',
      'model_config': {
          'gbc': {
              'n_estimators': 200,
              'learning_rate': 1.0,
              'max_depth': 6,
              'random_state': 0
          },
          'xgb': {
              'learning_rate': 0.01,
              'n_estimators': 81,
              'max_depth': 5,
              'min_child_weight': 1,
              'gamma': 0.4,
              'subsample': 0.7,
              'colsample_bytree': 1.0,
              'objective': 'binary:logistic',
              'nthread': 4,
              'scale_pos_weight': 1,
              'reg_alpha': 0.05
          }
      },
      # Settings for parameter search Grid or Random for different models
      'param_search': {
          'gbc': {
              # TODO If we want to also test gradient boosting (low prio)
          },
          'xgb': {
              # Searches for the best number of trees
              'perform_estimator_search': False,
              'estimator_search': {
                  # Number of cross validation steps
                  'nfold': 3,
                  # CV error needs to decrease at least every
                  # <early_stopping_rounds> round(s) to continue.
                  'early_stopping_rounds': 20,
              },
              # The type of parameter search to use random_search or
              # grid_search
              'search_type': 'grid_search',
              'grid_search': {
                  # Initial values taken from:
                  # https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/
                  'param_distributions': [
                      {
                          'max_depth': range(1, 6),
                          'min_child_weight': range(1, 6)

                      },
                      {
                          'gamma': [i / 10.0 for i in range(0, 5)]
                      },
                      {
                          'subsample': [i / 10.0 for i in range(6, 10)],
                      },
                      {
                          'reg_alpha': [0, 0.001, 0.005, 0.01, 0.05]
                      }
                  ],
                  # Score to optimize
                  'scoring': 'neg_log_loss',
                  # Number of cross validation steps per run
                  'cv': 5,
                  # Number of parallel search runs
                  'n_jobs': 4
              },
              'random_search': {
                  # Initial values taken from:
                  # https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/
                  'param_distributions': [
                      # Each dict represents one search run.
                      # After every run the model params will be updated with
                      # the newly found best parameters.
                      {
                          'max_depth': sk_randint(1, 6),
                          'min_child_weight': sk_randint(1, 6)
                      },
                      {
                          'gamma': [i / 10.0 for i in range(0, 5)]
                      },
                      {
                          'subsample': [i / 10.0 for i in range(6, 10)],
                      },
                      {
                          'reg_alpha': [0, 0.001, 0.005, 0.01, 0.05]
                      }
                  ],
                  # Number of parameter settings that are sampled
                  'n_iter': 3,
                  # Score to optimize
                  'scoring': 'neg_log_loss',
                  # Number of cross validation steps per run
                  'cv': 2,
                  # Number of parallel search runs
                  'n_jobs': 4
              }
          }
      },
      # CSV Parser setup
      'ignored_header': set(['uid', 'triggering_merchant_id'])
  }
